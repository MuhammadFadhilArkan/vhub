{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82b8a1c1-3318-4eee-993c-cade5df08a37",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b34a37f2-3d67-4705-9940-22abf56ce5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cacd5aec-74a4-4ed0-9a9c-63492124a7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_name, model_train_configs):\n",
    "    model = YOLO(model_name)\n",
    "    results = model.train(**model_train_configs)\n",
    "    return model, results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c303ce-a65a-44a6-88f5-92126bdcf849",
   "metadata": {},
   "source": [
    "Lets start by using default model configuration and small number of epoch first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ec7d787-9aa2-4694-9b56-c821f77c330b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.2.14 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.2.13  Python-3.11.7 torch-2.3.0+cu121 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.yaml, data=./config.yaml, epochs=10, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=cuda, workers=8, project=mask_detection_result, name=1, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=mask_detection_result\\1\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751897  ultralytics.nn.modules.head.Detect           [3, [64, 128, 256]]           \n",
      "YOLOv8n summary: 225 layers, 3011433 parameters, 3011417 gradients, 8.2 GFLOPs\n",
      "\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\NC\\vhub\\data\\silver\\labels\\train.cache... 642 images, 0 backgrounds, 0 corrupt: 100%|██████████| 642\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\NC\\vhub\\data\\silver\\labels\\val.cache... 211 images, 0 backgrounds, 0 corrupt: 100%|██████████| 211/211\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to mask_detection_result\\1\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001429, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mmask_detection_result\\1\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/10      2.48G      3.694      5.108      4.184          4        640: 100%|██████████| 41/41 [00:13<00:00,  2.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:03<0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        211        878   0.000153     0.0166   9.53e-05   1.91e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/10      2.52G      3.082      4.506      3.688         11        640: 100%|██████████| 41/41 [00:09<00:00,  4.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:01<0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        211        878   0.000158    0.00359    8.1e-05   1.01e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/10      2.49G      2.579      3.644      2.746          5        640: 100%|██████████| 41/41 [00:09<00:00,  4.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        211        878      0.675     0.0353    0.00417     0.0017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/10      2.44G      2.431      3.116      2.383          2        640: 100%|██████████| 41/41 [00:09<00:00,  4.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        211        878      0.685     0.0266     0.0104     0.0044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/10      2.48G      2.205      2.678      2.092          2        640: 100%|██████████| 41/41 [00:09<00:00,  4.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        211        878      0.871      0.137      0.166     0.0776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/10      2.67G      2.035      2.314      1.965          7        640: 100%|██████████| 41/41 [00:09<00:00,  4.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        211        878      0.916      0.173      0.226       0.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/10      2.57G      1.928      2.113      1.829          9        640: 100%|██████████| 41/41 [00:09<00:00,  4.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        211        878      0.625      0.236      0.251      0.122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/10       2.6G      1.864       2.01      1.743         59        640: 100%|██████████| 41/41 [00:09<00:00,  4.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        211        878      0.937      0.202      0.287      0.153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/10      2.42G      1.815      1.866      1.713          6        640: 100%|██████████| 41/41 [00:09<00:00,  4.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        211        878      0.639      0.304      0.283      0.153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/10      2.51G      1.773      1.821      1.698          2        640: 100%|██████████| 41/41 [00:09<00:00,  4.\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        211        878      0.696      0.314      0.331      0.182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 epochs completed in 0.051 hours.\n",
      "Optimizer stripped from mask_detection_result\\1\\weights\\last.pt, 6.2MB\n",
      "Optimizer stripped from mask_detection_result\\1\\weights\\best.pt, 6.2MB\n",
      "\n",
      "Validating mask_detection_result\\1\\weights\\best.pt...\n",
      "Ultralytics YOLOv8.2.13  Python-3.11.7 torch-2.3.0+cu121 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB)\n",
      "YOLOv8n summary (fused): 168 layers, 3006233 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:03<0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        211        878      0.697      0.315      0.331      0.182\n",
      "          without_mask        211        156      0.608      0.159      0.232      0.111\n",
      "             with_mask        211        690      0.483      0.786      0.752      0.431\n",
      " mask_weared_incorrect        211         32          1          0     0.0102    0.00391\n",
      "Speed: 0.3ms preprocess, 3.3ms inference, 0.0ms loss, 2.1ms postprocess per image\n",
      "Results saved to \u001b[1mmask_detection_result\\1\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model_name = \"yolov8n.yaml\"\n",
    "model_train_configs = {\"data\": './config.yaml',\n",
    "                      \"epochs\": 10,\n",
    "                      \"device\": 'cuda',\n",
    "                      \"project\": 'mask_detection_result',\n",
    "                      \"name\": '1'\n",
    "                      }\n",
    "\n",
    "model, results = train_model(model_name, model_train_configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf010e9-b090-4602-8f18-fa33640c694f",
   "metadata": {},
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f5506f-d7fd-42bc-8497-f0cada5f9aa2",
   "metadata": {},
   "source": [
    "There are some metrics that can be used for evaluating the performance of object detection model, such as IoU, Average Precision (AP), Mean Average Precision mAP, Precision, Recall, F1 Score. In this case, we will be using mAP50-95 because it generally is the most comprehensive metrics that already incorporate most of the metrics mentioned above across a range of IoU thresholds from 0.5 to 0.95."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9cd38d08-45b1-4594-b452-e24df5730dde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP50-95:  0.18192993248145245\n"
     ]
    }
   ],
   "source": [
    "mAP50_95 = results.box.map\n",
    "print('mAP50-95: ', mAP50_95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3447c68a-e56b-42b7-95df-adeee57ff01d",
   "metadata": {},
   "source": [
    "#### Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751f7762-5d01-465f-9fc1-4dc7756205ec",
   "metadata": {},
   "source": [
    "In general, there are 2 main components that can be fine tuned to improve the models performance: \n",
    "1. Data Augmentation Parameter, such as Hue augmentation, Saturation augmentation, Value (brightness) augmentation, Rotation augmentation, Translation augmentation, Scaling augmentation, Shear augmentation, Perspective augmentation, Vertical flip augmentation, Horizontal flip augmentation, etc.\n",
    "2. Model Hyperparameter, such as learning rate, batch size, epochs number, optimizer, momentum, dropout factor, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80db34a2-063f-4a1e-a929-cf160ed34b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_model(model_name, model_tune_configs):\n",
    "    model = YOLO(model_name)\n",
    "    result_grid = model.tune(**model_tune_configs)\n",
    "    return model, result_grid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b40417e-6116-4a5b-9c71-bf7ab9ce66b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using Ray Tune to fine tune the momentum parameter, and increase the number of epoch.\n",
    "from ray import tune\n",
    "\n",
    "model_name = \"yolov8n.yaml\"\n",
    "param_for_tuning = {\"momentum\": tune.uniform(0.6, 0.98)}\n",
    "\n",
    "model_tune_configs = {\"data\": 'config.yaml',\n",
    "                      \"epochs\": 50,\n",
    "                      \"device\": 'cuda',\n",
    "                      \"project\": 'mask_detection_result',\n",
    "                      \"name\": '2',\n",
    "                      \"space\": param_for_tuning,\n",
    "                      \"use_ray\": True\n",
    "                      }\n",
    "\n",
    "model, results = tune_model(model_name, model_tune_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f51984-04ef-4028-b675-98c15abb0754",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mTuner: \u001b[0mInitialized Tuner instance with 'tune_dir=mask_detection_result\\tune'\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0m Learn about tuning at https://docs.ultralytics.com/guides/hyperparameter-tuning\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mStarting iteration 1/10 with hyperparameters: {'lr0': 0.01, 'lrf': 0.01, 'momentum': 0.937, 'weight_decay': 0.0005, 'warmup_epochs': 3.0, 'warmup_momentum': 0.8, 'box': 7.5, 'cls': 0.5, 'dfl': 1.5, 'hsv_h': 0.015, 'hsv_s': 0.7, 'hsv_v': 0.4, 'degrees': 0.0, 'translate': 0.1, 'scale': 0.5, 'shear': 0.0, 'perspective': 0.0, 'flipud': 0.0, 'fliplr': 0.5, 'bgr': 0.0, 'mosaic': 1.0, 'mixup': 0.0, 'copy_paste': 0.0}\n",
      "Saved mask_detection_result\\tune\\tune_scatter_plots.png\n",
      "Saved mask_detection_result\\tune\\tune_fitness.png\n",
      "\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0m1/10 iterations complete  (436.76s)\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mResults saved to \u001b[1mmask_detection_result\\tune\u001b[0m\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness=0.51625 observed at iteration 1\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness metrics are {'metrics/precision(B)': 0.71699, 'metrics/recall(B)': 0.7058, 'metrics/mAP50(B)': 0.73581, 'metrics/mAP50-95(B)': 0.49185, 'val/box_loss': 1.10463, 'val/cls_loss': 0.69068, 'val/dfl_loss': 1.1741, 'fitness': 0.51625}\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness model is mask_detection_result\\2\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness hyperparameters are printed below.\n",
      "\n",
      "Printing '\u001b[1m\u001b[30mmask_detection_result\\tune\\best_hyperparameters.yaml\u001b[0m'\n",
      "\n",
      "lr0: 0.01\n",
      "lrf: 0.01\n",
      "momentum: 0.937\n",
      "weight_decay: 0.0005\n",
      "warmup_epochs: 3.0\n",
      "warmup_momentum: 0.8\n",
      "box: 7.5\n",
      "cls: 0.5\n",
      "dfl: 1.5\n",
      "hsv_h: 0.015\n",
      "hsv_s: 0.7\n",
      "hsv_v: 0.4\n",
      "degrees: 0.0\n",
      "translate: 0.1\n",
      "scale: 0.5\n",
      "shear: 0.0\n",
      "perspective: 0.0\n",
      "flipud: 0.0\n",
      "fliplr: 0.5\n",
      "bgr: 0.0\n",
      "mosaic: 1.0\n",
      "mixup: 0.0\n",
      "copy_paste: 0.0\n",
      "\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mStarting iteration 2/10 with hyperparameters: {'lr0': 0.00997, 'lrf': 0.00925, 'momentum': 0.937, 'weight_decay': 0.00047, 'warmup_epochs': 2.80855, 'warmup_momentum': 0.79991, 'box': 7.47642, 'cls': 0.5, 'dfl': 1.5, 'hsv_h': 0.015, 'hsv_s': 0.68568, 'hsv_v': 0.4, 'degrees': 0.0, 'translate': 0.1027, 'scale': 0.47718, 'shear': 0.0, 'perspective': 0.0, 'flipud': 0.0, 'fliplr': 0.5, 'bgr': 0.0, 'mosaic': 0.99146, 'mixup': 0.0, 'copy_paste': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# Fine tune the optimizer parameter, and increase the number of epoch.\n",
    "\n",
    "model_name = \"yolov8n.yaml\"\n",
    "\n",
    "model_tune_configs = {\"data\": 'config.yaml',\n",
    "                      \"epochs\": 50,\n",
    "                      \"device\": 'cuda',\n",
    "                      \"project\": 'mask_detection_result',\n",
    "                      \"name\": '2',\n",
    "                      \"optimizer\": 'AdamW',\n",
    "                      }\n",
    "\n",
    "model, results = tune_model(model_name, model_tune_configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8f8215-97c1-422d-ad35-0c021ef17c69",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd745d2-4efd-4208-accd-e40fc9e1da66",
   "metadata": {},
   "source": [
    "To even improve the model performance, \n",
    "1. we can use SAHI (Slicing Aided Hyper Inference) so that the model will be able to detect smller objects.\n",
    "2. export model to different format such as ONNX to improve inference speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61196985-73a3-4260-ad3d-eeccf2713d8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630205f0-a9ee-4c80-b377-da2b94a0dc58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8b180a-1c5c-4989-b42b-482c498d2110",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c7b3cb-cccf-40a0-91ee-329909f250ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8071e3d-734f-44cd-9680-9e5c97bfa016",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b681474-0d37-4671-8209-f589223cf60d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
